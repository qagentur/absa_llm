
@misc{scaria_instructabsa_2023,
	title = {{InstructABSA}: {Instruction} {Learning} for {Aspect} {Based} {Sentiment} {Analysis}},
	shorttitle = {{InstructABSA}},
	url = {http://arxiv.org/abs/2302.08624},
	doi = {10.48550/arXiv.2302.08624},
	abstract = {In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for the ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) the ABSA subtasks, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on the three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 5.69\% points, Rest15 ATSC subtask by 9.59\% points, and on the Lapt14 Joint Task by 3.37\% points. Our results also suggest a strong generalization ability to new domains across all three subtasks},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Scaria, Kevin and Gupta, Himanshu and Goyal, Siddharth and Sawant, Saurabh Arjun and Mishra, Swaroop and Baral, Chitta},
	month = apr,
	year = {2023},
	note = {arXiv:2302.08624 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 4 pages, 2 figures, 5 tables, 5 appendix pages},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/J26GJ73E/Scaria et al. - 2023 - InstructABSA Instruction Learning for Aspect Base.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/2WT42KDN/2302.html:text/html},
}

@inproceedings{pontiki_semeval-2014_2014,
	address = {Dublin, Ireland},
	title = {{SemEval}-2014 {Task} 4: {Aspect} {Based} {Sentiment} {Analysis}},
	shorttitle = {{SemEval}-2014 {Task} 4},
	url = {https://aclanthology.org/S14-2004},
	doi = {10.3115/v1/S14-2004},
	urldate = {2023-05-20},
	booktitle = {Proceedings of the 8th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2014)},
	publisher = {Association for Computational Linguistics},
	author = {Pontiki, Maria and Galanis, Dimitris and Pavlopoulos, John and Papageorgiou, Harris and Androutsopoulos, Ion and Manandhar, Suresh},
	month = aug,
	year = {2014},
	pages = {27--35},
	file = {Full Text PDF:/Users/paulsimmering/Zotero/storage/L7JQ422A/Pontiki et al. - 2014 - SemEval-2014 Task 4 Aspect Based Sentiment Analys.pdf:application/pdf},
}

@article{do_deep_2019,
	title = {Deep {Learning} for {Aspect}-{Based} {Sentiment} {Analysis}: {A} {Comparative} {Review}},
	volume = {118},
	issn = {0957-4174},
	shorttitle = {Deep {Learning} for {Aspect}-{Based} {Sentiment} {Analysis}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417418306456},
	doi = {10.1016/j.eswa.2018.10.003},
	abstract = {The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the human emotional state. A current research focus for sentiment analysis is the improvement of granularity at aspect level, representing two distinct aims: aspect extraction and sentiment classification of product reviews and sentiment classification of target-dependent tweets. Deep learning approaches have emerged as a prospect for achieving these aims with their ability to capture both syntactic and semantic features of text without requirements for high-level feature engineering, as is the case in earlier methods. In this article, we aim to provide a comparative review of deep learning for aspect-based sentiment analysis to place different approaches in context.},
	language = {en},
	urldate = {2023-05-20},
	journal = {Expert Systems with Applications},
	author = {Do, Hai Ha and Prasad, PWC and Maag, Angelika and Alsadoon, Abeer},
	month = mar,
	year = {2019},
	pages = {272--299},
	file = {Do et al. - 2019 - Deep Learning for Aspect-Based Sentiment Analysis.pdf:/Users/paulsimmering/Zotero/storage/XC74LELL/Do et al. - 2019 - Deep Learning for Aspect-Based Sentiment Analysis.pdf:application/pdf;ScienceDirect Snapshot:/Users/paulsimmering/Zotero/storage/TX6FUB39/S0957417418306456.html:text/html},
}

@misc{yang_pyabsa_2023,
	title = {{PyABSA}: {A} {Modularized} {Framework} for {Reproducible} {Aspect}-based {Sentiment} {Analysis}},
	shorttitle = {{PyABSA}},
	url = {http://arxiv.org/abs/2208.01368},
	doi = {10.48550/arXiv.2208.01368},
	abstract = {The advancement of aspect-based sentiment analysis (ABSA) has urged the lack of a user-friendly framework that can largely lower the difficulty of reproducing state-of-the-art ABSA performance, especially for beginners. To meet the demand, we present {\textbackslash}our, a modularized framework built on PyTorch for reproducible ABSA. To facilitate ABSA research, PyABSA supports several ABSA subtasks, including aspect term extraction, aspect sentiment classification, and end-to-end aspect-based sentiment analysis. Concretely, PyABSA integrates 29 models and 26 datasets. With just a few lines of code, the result of a model on a specific dataset can be reproduced. With a modularized design, PyABSA can also be flexiblely extended to considered models, datasets, and other related tasks. Besides, PyABSA highlights its data augmentation and annotation features, which significantly address data scarity. All are welcome to have a try at {\textbackslash}url\{https://github.com/yangheng95/PyABSA\}.},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Yang, Heng and Li, Ke},
	month = feb,
	year = {2023},
	note = {arXiv:2208.01368 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/SI6VG592/Yang and Li - 2023 - PyABSA A Modularized Framework for Reproducible A.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/Z4LSLFR4/2208.html:text/html},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}: {Tasks}, {Methods}, and {Challenges}},
	issn = {1558-2191},
	shorttitle = {A {Survey} on {Aspect}-{Based} {Sentiment} {Analysis}},
	doi = {10.1109/TKDE.2022.3230975},
	abstract = {As an important fine-grained sentiment analysis problem, aspect-based sentiment analysis (ABSA), aiming to analyze and understand people's opinions at the aspect level, has been attracting considerable interest in the last decade. To handle ABSA in different scenarios, various tasks are introduced for analyzing different sentiment elements and their relations, including the aspect term, aspect category, opinion term, and sentiment polarity. Unlike early ABSA works focusing on a single sentiment element, many compound ABSA tasks involving multiple elements have been studied in recent years for capturing more complete aspect-level sentiment information. However, a systematic review of various ABSA tasks and their corresponding solutions is still lacking, which we aim to fill in this survey. More specifically, we provide a new taxonomy for ABSA which organizes existing studies from the axes of concerned sentiment elements, with an emphasis on recent advances of compound ABSA tasks. From the perspective of solutions, we summarize the utilization of pre-trained language models for ABSA, which improved the performance of ABSA to a new stage. Besides, techniques for building more practical ABSA systems in cross-domain/lingual scenarios are discussed. Finally, we review some emerging topics and discuss some open challenges to outlook potential future directions of ABSA.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Wenxuan and Li, Xin and Deng, Yang and Bing, Lidong and Lam, Wai},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Analytical models, Aspect-Based Sentiment Analysis, Compounds, Data mining, Opinion Mining, Pre-trained Language Models, Sentiment analysis, Sentiment Analysis, Systematics, Task analysis, Taxonomy},
	pages = {1--20},
	file = {IEEE Xplore Abstract Record:/Users/paulsimmering/Zotero/storage/IFCZ5X3R/9996141.html:text/html;Submitted Version:/Users/paulsimmering/Zotero/storage/VZ9VHSWN/Zhang et al. - 2022 - A Survey on Aspect-Based Sentiment Analysis Tasks.pdf:application/pdf},
}

@article{wankhade_survey_2022,
	title = {A survey on sentiment analysis methods, applications, and challenges},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10144-1},
	doi = {10.1007/s10462-022-10144-1},
	abstract = {The rapid growth of Internet-based applications, such as social media platforms and blogs, has resulted in comments and reviews concerning day-to-day activities. Sentiment analysis is the process of gathering and analyzing people’s opinions, thoughts, and impressions regarding various topics, products, subjects, and services. People’s opinions can be beneficial to corporations, governments, and individuals for collecting information and making decisions based on opinion. However, the sentiment analysis and evaluation procedure face numerous challenges. These challenges create impediments to accurately interpreting sentiments and determining the appropriate sentiment polarity. Sentiment analysis identifies and extracts subjective information from the text using natural language processing and text mining. This article discusses a complete overview of the method for completing this task as well as the applications of sentiment analysis. Then, it evaluates, compares, and investigates the approaches used to gain a comprehensive understanding of their advantages and disadvantages. Finally, the challenges of sentiment analysis are examined in order to define future directions.},
	language = {en},
	number = {7},
	urldate = {2023-05-20},
	journal = {Artificial Intelligence Review},
	author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},
	month = oct,
	year = {2022},
	keywords = {Sentiment analysis, Machine learning, Social media, Text analysis, Word embedding},
	pages = {5731--5780},
	file = {Full Text PDF:/Users/paulsimmering/Zotero/storage/74N9BE4U/Wankhade et al. - 2022 - A survey on sentiment analysis methods, applicatio.pdf:application/pdf},
}

@book{liu_sentiment_2020,
	title = {Sentiment {Analysis}: {Mining} {Opinions}, {Sentiments}, and {Emotions}},
	isbn = {978-1-108-78728-4},
	shorttitle = {Sentiment {Analysis}},
	abstract = {Sentiment analysis is the computational study of people's opinions, sentiments, emotions, moods, and attitudes. This fascinating problem offers numerous research challenges, but promises insight useful to anyone interested in opinion analysis and social media analysis. This comprehensive introduction to the topic takes a natural-language-processing point of view to help readers understand the underlying structure of the problem and the language constructs commonly used to express opinions, sentiments, and emotions. The book covers core areas of sentiment analysis and also includes related topics such as debate analysis, intention mining, and fake-opinion detection. It will be a valuable resource for researchers and practitioners in natural language processing, computer science, management sciences, and the social sciences. In addition to traditional computational methods, this second edition includes recent deep learning methods to analyze and summarize sentiments and opinions, and also new material on emotion and mood analysis techniques, emotion-enhanced dialogues, and multimodal emotion analysis.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Liu, Bing},
	month = oct,
	year = {2020},
	note = {Google-Books-ID: v0UBEAAAQBAJ},
	keywords = {Business \& Economics / Advertising \& Promotion, Computers / Artificial Intelligence / General, Computers / Artificial Intelligence / Natural Language Processing, Computers / Data Science / Data Analytics, Computers / System Administration / Storage \& Retrieval, Political Science / Political Process / General, Psychology / Emotions, Reference / Research, Social Science / Media Studies},
}

@inproceedings{wang_super-naturalinstructions_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Super-{NaturalInstructions}: {Generalization} via {Declarative} {Instructions} on 1600+ {NLP} {Tasks}},
	shorttitle = {Super-{NaturalInstructions}},
	url = {https://aclanthology.org/2022.emnlp-main.340},
	abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9\% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
	urldate = {2023-05-20},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Pal, Kuntal Kumar and Patel, Maitreya and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Doshi, Savan and Sampat, Shailaja Keyur and Mishra, Siddhartha and Reddy A, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong},
	month = dec,
	year = {2022},
	pages = {5085--5109},
	file = {Full Text PDF:/Users/paulsimmering/Zotero/storage/VV3E2FG8/Wang et al. - 2022 - Super-NaturalInstructions Generalization via Decl.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/V8GWYND5/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/L4RU3PEL/1810.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 100 pages},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/MBE68AUE/OpenAI - 2023 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/4WW6EYH5/2303.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2023-05-20},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/GN2VVNNZ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/CZ2Y2EKI/2005.html:text/html},
}

@misc{he_debertav3_2023,
	title = {{DeBERTaV3}: {Improving} {DeBERTa} using {ELECTRA}-{Style} {Pre}-{Training} with {Gradient}-{Disentangled} {Embedding} {Sharing}},
	shorttitle = {{DeBERTaV3}},
	url = {http://arxiv.org/abs/2111.09543},
	doi = {10.48550/arXiv.2111.09543},
	abstract = {This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37\% average score, which is 1.37\% over DeBERTa and 1.91\% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8\% zero-shot cross-lingual accuracy on XNLI and a 3.6\% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa.},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
	month = mar,
	year = {2023},
	note = {arXiv:2111.09543 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, cs.CL, cs.GL, I.2, I.7},
	annote = {Comment: 16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly improves performance of the downstream NLU tasks over models with a similar structure, e.g. DeBERTaV3 large achieves 91.37\% average GLUE score which is 1.37\% over DeBERTa large. XSmall has only 22M backbone parameters, but significantly outperforms RoBERTa/XLNet-base. Paper is published as a conference paper at ICLR 2023},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/CBIDS9H3/He et al. - 2023 - DeBERTaV3 Improving DeBERTa using ELECTRA-Style P.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/IE6PUB6W/2111.html:text/html},
}


@misc{min_rethinking_2022,
	title = {Rethinking the {Role} of {Demonstrations}: {What} {Makes} {In}-{Context} {Learning} {Work}?},
	shorttitle = {Rethinking the {Role} of {Demonstrations}},
	url = {http://arxiv.org/abs/2202.12837},
	abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
	urldate = {2023-07-16},
	publisher = {arXiv},
	author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
	month = oct,
	year = {2022},
	note = {arXiv:2202.12837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/SD8LJWMK/2202.html:text/html;Full Text PDF:/Users/paulsimmering/Zotero/storage/LIIFNT34/Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf:application/pdf},
}


@article{liu_pre-train_2023,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	volume = {55},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {https://dl.acm.org/doi/10.1145/3560815},
	doi = {10.1145/3560815},
	abstract = {This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input
              
                x
              
              and predict an output
              
                y
              
              as
              P
              (
              
                y{\textbar}x
              
              ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input
              
                x
              
              is modified using a
              template
              into a textual string
              prompt
              
                x′
              
              that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string
              
                x̂
              
              , from which the final output
              
                y
              
              can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be
              pre-trained
              on massive amounts of raw text, and by defining a new prompting function the model is able to perform
              few-shot
              or even
              zero-shot
              learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website
              NLPedia–Pretrain
              including constantly updated survey and paperlist.},
	language = {en},
	number = {9},
	urldate = {2023-07-16},
	journal = {ACM Computing Surveys},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = sep,
	year = {2023},
	pages = {1--35},
	file = {Full Text PDF:/Users/paulsimmering/Zotero/storage/D4KEKKKL/Liu et al. - 2023 - Pre-train, Prompt, and Predict A Systematic Surve.pdf:application/pdf},
}



@misc{zhang_sentiment_2023,
	title = {Sentiment {Analysis} in the {Era} of {Large} {Language} {Models}: {A} {Reality} {Check}},
	shorttitle = {Sentiment {Analysis} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2305.15005},
	abstract = {Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, {\textbackslash}textsc\{SentiEval\}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at {\textbackslash}url\{https://github.com/DAMO-NLP-SG/LLM-Sentiment\}.},
	urldate = {2023-07-24},
	publisher = {arXiv},
	author = {Zhang, Wenxuan and Deng, Yue and Liu, Bing and Pan, Sinno Jialin and Bing, Lidong},
	month = may,
	year = {2023},
	note = {arXiv:2305.15005 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/4P9KT7TC/2305.html:text/html;Full Text PDF:/Users/paulsimmering/Zotero/storage/7F2J8B3C/Zhang et al. - 2023 - Sentiment Analysis in the Era of Large Language Mo.pdf:application/pdf},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-09-24},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/paulsimmering/Zotero/storage/JSCJZ2FY/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/Users/paulsimmering/Zotero/storage/DWJK6666/2307.html:text/html},
}

@article{DBLP:journals/corr/abs-2005-14165,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  journal      = {CoRR},
  volume       = {abs/2005.14165},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.14165},
  eprinttype    = {arXiv},
  eprint       = {2005.14165},
  timestamp    = {Thu, 25 May 2023 10:38:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{turney2002,
author = {Turney, Peter D.},
title = {Thumbs up or Thumbs down? Semantic Orientation Applied to Unsupervised Classification of Reviews},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073153},
doi = {10.3115/1073083.1073153},
abstract = {This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., "subtle nuances") and a negative semantic orientation when it has bad associations (e.g., "very cavalier"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word "excellent" minus the mutual information between the given phrase and the word "poor". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74\% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84\% for automobile reviews to 66\% for movie reviews.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {417–424},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@inproceedings{pang-etal-2002-thumbs,
    title = "Thumbs up? Sentiment Classification using Machine Learning Techniques",
    author = "Pang, Bo  and
      Lee, Lillian  and
      Vaithyanathan, Shivakumar",
    booktitle = "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002)",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-1011",
    doi = "10.3115/1118693.1118704",
    pages = "79--86",
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mao2023gpteval,
      title={GPTEval: A Survey on Assessments of ChatGPT and GPT-4}, 
      author={Rui Mao and Guanyi Chen and Xulang Zhang and Frank Guerin and Erik Cambria},
      year={2023},
      eprint={2308.12488},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{bengio_curriculum_2009,
	address = {Montreal Quebec Canada},
	title = {Curriculum learning},
	isbn = {978-1-60558-516-1},
	url = {https://dl.acm.org/doi/10.1145/1553374.1553380},
	doi = {10.1145/1553374.1553380},
	language = {en},
	urldate = {2023-10-20},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Bengio, Yoshua and Louradour, Jérôme and Collobert, Ronan and Weston, Jason},
	month = jun,
	year = {2009},
	pages = {41--48},
	file = {Available Version (via Google Scholar):/Users/paulsimmering/Zotero/storage/K44VRTLU/Bengio et al. - 2009 - Curriculum learning.pdf:application/pdf},
}

@misc{liu2021makes,
      title={What Makes Good In-Context Examples for GPT-$3$?}, 
      author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
      year={2021},
      eprint={2101.06804},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2021makes,
      title={What Makes Good In-Context Examples for GPT-$3$?}, 
      author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
      year={2021},
      eprint={2101.06804},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
